import { SystemMessage } from "@langchain/core/messages";
import type { MessagesAnnotation } from "@langchain/langgraph";
import { createModel } from "../config/model-config";
import { SYSTEM_PROMPT_TEMPLATE } from "../prompts/system-prompt";
import { formatCurrentDate, formatCurrentTime, buildDynamicSystemPrompt } from "../utils/date-formatters";
import { logger } from "@/lib/utils/logger";

/**
 * Noeud de l'agent : appelle le modÃ¨le LLM avec le prompt systÃ¨me dynamique
 * @param state Ã‰tat du graphe contenant les messages
 * @returns Nouvel Ã©tat avec la rÃ©ponse du modÃ¨le
 */
export async function callModel(state: typeof MessagesAnnotation.State) {
  const { messages } = state;
  const model = createModel();

  // A. Calcul du temps prÃ©sent
  const currentDate = formatCurrentDate();
  const currentTime = formatCurrentTime();

  // B. Construction du Prompt SystÃ¨me Dynamique
  const dynamicSystemPrompt = buildDynamicSystemPrompt(
    SYSTEM_PROMPT_TEMPLATE,
    currentDate,
    currentTime
  );

  // C. Fusion : System Prompt + Historique de conversation
  // On ajoute le system prompt au dÃ©but de la liste des messages envoyÃ©s Ã  Gemini
  // Note: LangChain gÃ¨re cela intelligemment sans Ã©craser l'historique visible

  // ğŸ” DEBUG: Ce que le LLM reÃ§oit
  logger.debug(`\nğŸ§  [AGENT NODE] RÃ©flexion en cours...`);
  logger.debug(`ğŸ“¨ [AGENT NODE] Messages entrants (${messages.length} total):`);
  messages.forEach((msg, i) => {
    const type = msg.constructor.name;
    const contentPreview = typeof msg.content === 'string'
      ? msg.content.substring(0, 150).replace(/\n/g, ' ')
      : JSON.stringify(msg.content).substring(0, 150);
    logger.debug(`  [${i}] ${type}: ${contentPreview}${contentPreview.length >= 150 ? '...' : ''}`);
    if ((msg as any).tool_calls?.length) {
      logger.debug(`      âš™ï¸  Tool calls demandÃ©s: ${(msg as any).tool_calls.map((tc: any) => tc.name).join(', ')}`);
    }
  });

  const result = await model.invoke([
    new SystemMessage(dynamicSystemPrompt),
    ...messages
  ]);

  // ğŸ” DEBUG: Ce que le LLM rÃ©pond
  logger.debug(`\nğŸ’­ [AGENT NODE] RÃ©ponse du LLM:`);
  logger.debug(`   Type: ${result.constructor.name}`);
  const resultContent = typeof result.content === 'string' ? result.content : JSON.stringify(result.content);
  logger.debug(`   Content: ${resultContent.substring(0, 200)}${resultContent.length > 200 ? '...' : ''}`);
  if (result.tool_calls?.length) {
    logger.debug(`   ğŸ› ï¸  DÃ‰CISION: Appeler les outils â†’ ${result.tool_calls.map((tc: any) => tc.name).join(', ')}`);
  } else {
    logger.debug(`   âœ‹ DÃ‰CISION: ArrÃªt (pas d'outil Ã  appeler, rÃ©ponse finale)`);
  }

  return { messages: [result] };
}

